import numpy as np
import cv2
from shapely.geometry import Polygon
import pyclipper


class DBPostProcessONNX(object):
    """
    DB postprocessing for ONNX models - converted from PaddleOCR
    
    DB (Differentiable Binarization) l√† thu·∫≠t to√°n text detection chuy·ªÉn ƒë·ªïi 
    probability map th√†nh binary mask, sau ƒë√≥ t√¨m contours ƒë·ªÉ extract bounding boxes.
    
    Hand-traced example:
    Input: ONNX detection output shape (1,1,320,480) v·ªõi probability values [0,1]
    Flow: probability_map ‚Üí binary_mask ‚Üí contours ‚Üí boxes ‚Üí final_boxes
    """

    def __init__(self,
                 thresh=0.3,         # Ng∆∞·ª°ng ƒë·ªÉ t·∫°o binary mask t·ª´ probability map
                 box_thresh=0.7,     # Ng∆∞·ª°ng confidence ƒë·ªÉ filter boxes
                 max_candidates=1000, # S·ªë l∆∞·ª£ng contours t·ªëi ƒëa ƒë∆∞·ª£c x·ª≠ l√Ω
                 unclip_ratio=2.0,   # T·ª∑ l·ªá m·ªü r·ªông box ƒë·ªÉ bao tr·ªçn text
                 use_dilation=False, # C√≥ √°p d·ª•ng morphological dilation kh√¥ng
                 score_mode="fast",  # C√°ch t√≠nh score: "fast" (bbox) vs "slow" (polygon)
                 box_type='quad',    # Lo·∫°i box: 'quad' (4 ƒëi·ªÉm) vs 'poly' (ƒëa gi√°c)
                 **kwargs):
        
        # EXAMPLE VALUES - Hand tracing v·ªõi nh·ªØng gi√° tr·ªã n√†y:
        # thresh=0.3: pixel > 0.3 ‚Üí white (1), pixel ‚â§ 0.3 ‚Üí black (0)
        # box_thresh=0.7: ch·ªâ gi·ªØ boxes c√≥ confidence > 0.7
        # unclip_ratio=2.0: m·ªü r·ªông box g·∫•p 2 l·∫ßn ƒë·ªÉ bao tr·ªçn text
        
        self.thresh = thresh
        self.box_thresh = box_thresh
        self.max_candidates = max_candidates
        self.unclip_ratio = unclip_ratio
        self.min_size = 3              # Box nh·ªè h∆°n 3px s·∫Ω b·ªã lo·∫°i b·ªè
        self.score_mode = score_mode
        self.box_type = box_type
        
        assert score_mode in ["slow", "fast"], f"Score mode must be in [slow, fast] but got: {score_mode}"
        
        # Dilation kernel ƒë·ªÉ l√†m d√†y binary mask (optional)
        # [[1,1],[1,1]] nghƒ©a l√† m·ªói pixel s·∫Ω lan sang 4 pixel xung quanh
        self.dilation_kernel = None if not use_dilation else np.array([[1, 1], [1, 1]])

    def polygons_from_bitmap(self, pred, _bitmap, dest_width, dest_height):
        """
        Extract polygons from binary bitmap
        """
        bitmap = _bitmap
        height, width = bitmap.shape

        boxes = []
        scores = []

        contours, _ = cv2.findContours(
            (bitmap * 255).astype(np.uint8),
            cv2.RETR_LIST,
            cv2.CHAIN_APPROX_SIMPLE
        )

        for contour in contours[:self.max_candidates]:
            epsilon = 0.002 * cv2.arcLength(contour, True)
            approx = cv2.approxPolyDP(contour, epsilon, True)
            points = approx.reshape((-1, 2))
            
            if points.shape[0] < 4:
                continue

            score = self.box_score_fast(pred, points.reshape(-1, 2))
            if self.box_thresh > score:
                continue

            if points.shape[0] > 2:
                box = self.unclip(points, self.unclip_ratio)
                if len(box) > 1:
                    continue
            else:
                continue
                
            box = box.reshape(-1, 2)

            _, sside = self.get_mini_boxes(box.reshape((-1, 1, 2)))
            if sside < self.min_size + 2:
                continue

            box = np.array(box)
            box[:, 0] = np.clip(
                np.round(box[:, 0] / width * dest_width), 0, dest_width)
            box[:, 1] = np.clip(
                np.round(box[:, 1] / height * dest_height), 0, dest_height)
            boxes.append(box.tolist())
            scores.append(score)
            
        return boxes, scores

    def boxes_from_bitmap(self, pred, _bitmap, dest_width, dest_height):
        """
        Extract rectangular boxes from binary bitmap
        
        HAND-TRACED EXAMPLE:
        Input: 
        - pred: probability map (320,480) v·ªõi values [0,1]
        - _bitmap: binary mask (320,480) v·ªõi values [0,1] 
        - dest_width=640, dest_height=400 (original image size)
        
        Flow:
        binary_mask ‚Üí findContours ‚Üí extract_boxes ‚Üí filter_by_score ‚Üí scale_to_original ‚Üí final_boxes
        """
        bitmap = _bitmap  # Binary mask ƒë√£ ƒë∆∞·ª£c threshold t·ª´ probability map
        height, width = bitmap.shape  # height=320, width=480 (detection resolution)

        # T√¨m contours t·ª´ binary mask
        # cv2.findContours t√¨m t·∫•t c·∫£ ƒë∆∞·ªùng vi·ªÅn (contours) c·ªßa v√πng tr·∫Øng trong mask
        # RETR_LIST: l·∫•y t·∫•t c·∫£ contours kh√¥ng quan t√¢m hierarchy
        # CHAIN_APPROX_SIMPLE: compress contour points (b·ªè qua points trung gian)
        outs = cv2.findContours(
            (bitmap * 255).astype(np.uint8),  # Convert [0,1] ‚Üí [0,255] ƒë·ªÉ OpenCV x·ª≠ l√Ω
            cv2.RETR_LIST,
            cv2.CHAIN_APPROX_SIMPLE
        )
        
        # Handle different OpenCV versions (3.x vs 4.x)
        # OpenCV 3.x: returns (image, contours, hierarchy)
        # OpenCV 4.x: returns (contours, hierarchy)
        if len(outs) == 3:
            img, contours, _ = outs[0], outs[1], outs[2]
        elif len(outs) == 2:
            contours, _ = outs[0], outs[1]

        # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng contours ƒë·ªÉ tr√°nh qu√° t·∫£i
        # VD: c√≥ 1000 contours nh∆∞ng ch·ªâ x·ª≠ l√Ω 1000 c√°i ƒë·∫ßu ti√™n
        num_contours = min(len(contours), self.max_candidates)

        boxes = []    # Danh s√°ch c√°c boxes cu·ªëi c√πng
        scores = []   # Confidence score t∆∞∆°ng ·ª©ng v·ªõi m·ªói box
        
        # HAND-TRACE: Gi·∫£ s·ª≠ c√≥ 3 contours ƒë∆∞·ª£c t√¨m th·∫•y
        # Contour 1: text "HELLO" ·ªü (100,50) size 80x30
        # Contour 2: text "WORLD" ·ªü (200,100) size 60x25  
        # Contour 3: noise blob ·ªü (10,10) size 5x5
        
        for index in range(num_contours):
            contour = contours[index]  # L·∫•y contour th·ª© index
            
            # T√¨m minimum bounding rectangle c·ªßa contour
            # Tr·∫£ v·ªÅ 4 ƒëi·ªÉm g√≥c c·ªßa rectangle v√† chi·ªÅu d√†i c·∫°nh ng·∫Øn nh·∫•t
            points, sside = self.get_mini_boxes(contour)
            
            # HAND-TRACE Contour 1: points=[(100,50), (180,50), (180,80), (100,80)], sside=30
            # HAND-TRACE Contour 2: points=[(200,100), (260,100), (260,125), (200,125)], sside=25
            # HAND-TRACE Contour 3: points=[(10,10), (15,10), (15,15), (10,15)], sside=5
            
            # Filter out boxes qu√° nh·ªè (likely noise)
            if sside < self.min_size:  # min_size=3
                continue  # Contour 3 b·ªã lo·∫°i v√¨ sside=5 < 3 (kh√¥ng ƒë√∫ng, nh∆∞ng gi·∫£ s·ª≠ min_size=10)
                
            points = np.array(points)
            
            # T√≠nh confidence score c·ªßa box n√†y
            if self.score_mode == "fast":
                # Fast mode: t√≠nh mean c·ªßa probability trong bounding rectangle
                score = self.box_score_fast(pred, points.reshape(-1, 2))
            else:
                # Slow mode: t√≠nh mean c·ªßa probability trong exact contour polygon
                score = self.box_score_slow(pred, contour)
                
            # HAND-TRACE: 
            # Contour 1 score = mean of pred[50:80, 100:180] = 0.85
            # Contour 2 score = mean of pred[100:125, 200:260] = 0.65
            
            # Filter boxes v·ªõi confidence th·∫•p
            if self.box_thresh > score:  # box_thresh=0.7
                continue  # Contour 2 b·ªã lo·∫°i v√¨ 0.65 < 0.7
                
            # M·ªü r·ªông box ƒë·ªÉ bao tr·ªçn text ho√†n to√†n (v√¨ text c√≥ th·ªÉ b·ªã c·∫Øt)
            # unclip_ratio=2.0 nghƒ©a l√† m·ªü r·ªông box theo t·ª∑ l·ªá n√†y
            box = self.unclip(points, self.unclip_ratio).reshape(-1, 1, 2)
            box, sside = self.get_mini_boxes(box)
            
            # HAND-TRACE Contour 1 sau unclip: 
            # points t·ª´ [(100,50), (180,50), (180,80), (100,80)]
            # th√†nh [(90,40), (190,40), (190,90), (90,90)] (m·ªü r·ªông ~10px m·ªói ph√≠a)
            
            # Filter out boxes v·∫´n qu√° nh·ªè sau khi unclip
            if sside < self.min_size + 2:  # min_size + 2 = 5
                continue
                
            box = np.array(box)

            # Scale boxes t·ª´ detection resolution v·ªÅ original image resolution
            # detection: 320x480, original: 400x640
            # scale_x = dest_width/width = 640/480 = 1.33
            # scale_y = dest_height/height = 400/320 = 1.25
            box[:, 0] = np.clip(
                np.round(box[:, 0] / width * dest_width), 0, dest_width)
            box[:, 1] = np.clip(
                np.round(box[:, 1] / height * dest_height), 0, dest_height)
                
            # HAND-TRACE Contour 1 final scaling:
            # [(90,40), (190,40), (190,90), (90,90)] ‚Üí [(120,50), (253,50), (253,112), (120,112)]
            # x: 90*1.33=120, 190*1.33=253
            # y: 40*1.25=50, 90*1.25=112
            
            boxes.append(box.astype("int32"))  # Convert to integer coordinates
            scores.append(score)               # Store confidence score
            
        # FINAL RESULT: 1 box detected v·ªõi coordinates v√† score
        return np.array(boxes, dtype="int32"), scores

    def unclip(self, box, unclip_ratio):
        """
        Expand box using polygon offset
        
        T·∫°i sao c·∫ßn unclip?
        - Text detection model c√≥ th·ªÉ detect v√πng text h∆°i nh·ªè h∆°n th·ª±c t·∫ø
        - Unclip gi√∫p m·ªü r·ªông box ƒë·ªÉ bao tr·ªçn text ho√†n to√†n
        - Quan tr·ªçng ƒë·ªÉ recognition model nh·∫≠n ƒë·ªß context
        
        HAND-TRACED EXAMPLE:
        Input box: [(100,50), (180,50), (180,80), (100,80)] - rectangle 80x30
        unclip_ratio: 2.0
        
        Process:
        1. Create polygon t·ª´ 4 points
        2. Calculate expansion distance based on area/perimeter ratio
        3. Expand polygon outward
        4. Return expanded coordinates
        """
        poly = Polygon(box)  # T·∫°o polygon object t·ª´ coordinates
        
        # HAND-TRACE: poly.area = 80*30 = 2400, poly.length = 2*(80+30) = 220
        
        # Calculate distance ƒë·ªÉ expand
        # Formula: distance = (area * unclip_ratio) / perimeter
        # √ù t∆∞·ªüng: boxes l·ªõn expand nhi·ªÅu h∆°n, boxes nh·ªè expand √≠t h∆°n
        distance = poly.area * unclip_ratio / poly.length
        
        # HAND-TRACE: distance = 2400 * 2.0 / 220 = 21.8 pixels
        
        # S·ª≠ d·ª•ng PyCli0pper ƒë·ªÉ expand polygon
        offset = pyclipper.PyclipperOffset()
        offset.AddPath(box, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)
        expanded = np.array(offset.Execute(distance))
        
        # HAND-TRACE Result: expanded box s·∫Ω l·ªõn h∆°n ~22 pixels v·ªÅ m·ªói ph√≠a
        # [(100,50), (180,50), (180,80), (100,80)] 
        # ‚Üí [(78,28), (202,28), (202,102), (78,102)] (approximately)
        
        return expanded

    def get_mini_boxes(self, contour):
        """
        Get minimum area rectangle from contour
        
        T·∫°i sao c·∫ßn minimum area rectangle?
        - Contour c√≥ th·ªÉ l√† shape b·∫•t k·ª≥ (irregular)
        - C·∫ßn convert th√†nh rectangle chu·∫©n v·ªõi 4 g√≥c
        - Minimum area rectangle l√† rectangle nh·ªè nh·∫•t bao quanh contour
        
        HAND-TRACED EXAMPLE:
        Input contour: irregular shape points c·ªßa text "HELLO"
        Output: 4 corner points c·ªßa rectangle + shortest side length
        """
        # T√¨m minimum area rectangle bao quanh contour
        # Tr·∫£ v·ªÅ: ((center_x, center_y), (width, height), angle)
        bounding_box = cv2.minAreaRect(contour)
        
        # HAND-TRACE: bounding_box = ((140, 65), (80, 30), 0)
        # center=(140,65), size=(80,30), angle=0 (kh√¥ng xoay)
        
        # Convert th√†nh 4 corner points
        # boxPoints tr·∫£ v·ªÅ 4 g√≥c c·ªßa rectangle theo th·ª© t·ª± ng·∫´u nhi√™n
        points = sorted(list(cv2.boxPoints(bounding_box)), key=lambda x: x[0])
        
        # HAND-TRACE: boxPoints = [(100,50), (100,80), (180,50), (180,80)]
        # Sau sort by x: [(100,50), (100,80), (180,50), (180,80)]
        
        # S·∫Øp x·∫øp l·∫°i 4 points theo th·ª© t·ª±: top-left, top-right, bottom-right, bottom-left
        # M·ª•c ƒë√≠ch: ƒë·∫£m b·∫£o box coordinates nh·∫•t qu√°n
        index_1, index_2, index_3, index_4 = 0, 1, 2, 3
        
        # V·ªõi 2 points b√™n tr√°i (x nh·ªè nh·∫•t): points[0], points[1]
        # Point n√†o c√≥ y nh·ªè h∆°n ‚Üí top-left, point kia ‚Üí bottom-left
        if points[1][1] > points[0][1]:  # points[1] ·ªü d∆∞·ªõi points[0]
            index_1 = 0  # top-left
            index_4 = 1  # bottom-left
        else:
            index_1 = 1  # top-left  
            index_4 = 0  # bottom-left
            
        # T∆∞∆°ng t·ª± v·ªõi 2 points b√™n ph·∫£i: points[2], points[3]
        if points[3][1] > points[2][1]:  # points[3] ·ªü d∆∞·ªõi points[2]
            index_2 = 2  # top-right
            index_3 = 3  # bottom-right
        else:
            index_2 = 3  # top-right
            index_3 = 2  # bottom-right

        # HAND-TRACE:
        # points[0]=(100,50), points[1]=(100,80) ‚Üí y[1] > y[0] ‚Üí index_1=0, index_4=1
        # points[2]=(180,50), points[3]=(180,80) ‚Üí y[3] > y[2] ‚Üí index_2=2, index_3=3
        # Final order: [(100,50), (180,50), (180,80), (100,80)] (clockwise t·ª´ top-left)
        
        box = [
            points[index_1], points[index_2], points[index_3], points[index_4]
        ]
        
        # min(bounding_box[1]) = min(width, height) = shortest side length
        # HAND-TRACE: min(80, 30) = 30
        return box, min(bounding_box[1])

    def box_score_fast(self, bitmap, _box):
        """
        Calculate box score using bbox mean
        
        T√≠nh confidence score c·ªßa detected box b·∫±ng c√°ch l·∫•y mean probability 
        trong v√πng rectangle bao quanh box.
        
        T·∫°i sao c·∫ßn box score?
        - Filter out false positive detections
        - Ch·ªâ gi·ªØ l·∫°i boxes c√≥ confidence cao
        - Fast mode: t√≠nh tr√™n bounding rectangle (nhanh h∆°n)
        
        HAND-TRACED EXAMPLE:
        Input:
        - bitmap: probability map (320,480) v·ªõi values [0,1]
        - _box: [(100,50), (180,50), (180,80), (100,80)]
        
        Process: Extract region ‚Üí Create mask ‚Üí Calculate mean
        """
        h, w = bitmap.shape[:2]  # h=320, w=480
        box = _box.copy()        # Tr√°nh modify original box
        
        # T√¨m bounding rectangle c·ªßa box (c√≥ th·ªÉ l√† rotated rectangle)
        # L·∫•y min/max coordinates ƒë·ªÉ t·∫°o axis-aligned rectangle
        xmin = np.clip(np.floor(box[:, 0].min()).astype("int32"), 0, w - 1)
        xmax = np.clip(np.ceil(box[:, 0].max()).astype("int32"), 0, w - 1)
        ymin = np.clip(np.floor(box[:, 1].min()).astype("int32"), 0, h - 1)
        ymax = np.clip(np.ceil(box[:, 1].max()).astype("int32"), 0, h - 1)

        # HAND-TRACE:
        # box x coordinates: [100, 180, 180, 100] ‚Üí xmin=100, xmax=180
        # box y coordinates: [50, 50, 80, 80] ‚Üí ymin=50, ymax=80
        # Clipped to image bounds: xmin=100, xmax=180, ymin=50, ymax=80

        # T·∫°o mask cho v√πng b√™n trong box
        # Mask c√≥ k√≠ch th∆∞·ªõc b·∫±ng bounding rectangle
        mask = np.zeros((ymax - ymin + 1, xmax - xmin + 1), dtype=np.uint8)
        
        # HAND-TRACE: mask shape = (80-50+1, 180-100+1) = (31, 81)
        
        # Chuy·ªÉn box coordinates v·ªÅ coordinate system c·ªßa mask
        # (tr·ª´ ƒëi offset c·ªßa bounding rectangle)
        box[:, 0] = box[:, 0] - xmin  # Shift x v·ªÅ 0-based
        box[:, 1] = box[:, 1] - ymin  # Shift y v·ªÅ 0-based
        
        # HAND-TRACE: box becomes [(0,0), (80,0), (80,30), (0,30)]
        
        # Fill polygon trong mask
        # cv2.fillPoly: v·∫Ω polygon v·ªõi value=1 trong mask
        cv2.fillPoly(mask, box.reshape(1, -1, 2).astype("int32"), 1)
        
        # HAND-TRACE: mask now c√≥ shape (31,81) v·ªõi 1s inside box, 0s outside
        
        # T√≠nh mean c·ªßa probability values trong v√πng box
        # cv2.mean(src, mask): t√≠nh mean c·ªßa src ch·ªâ t·∫°i positions where mask=1
        mean_score = cv2.mean(bitmap[ymin:ymax + 1, xmin:xmax + 1], mask)[0]
        
        # HAND-TRACE: 
        # bitmap[50:81, 100:181] shape=(31,81) - v√πng probability t∆∞∆°ng ·ª©ng
        # cv2.mean ch·ªâ t√≠nh mean t·∫°i positions where mask=1
        # N·∫øu probability trong box = 0.85 trung b√¨nh ‚Üí mean_score = 0.85
        
        return mean_score

    def box_score_slow(self, bitmap, contour):
        """
        Calculate box score using polygon mean
        """
        h, w = bitmap.shape[:2]
        contour = contour.copy()
        contour = np.reshape(contour, (-1, 2))

        xmin = np.clip(np.min(contour[:, 0]), 0, w - 1)
        xmax = np.clip(np.max(contour[:, 0]), 0, w - 1)
        ymin = np.clip(np.min(contour[:, 1]), 0, h - 1)
        ymax = np.clip(np.max(contour[:, 1]), 0, h - 1)

        mask = np.zeros((ymax - ymin + 1, xmax - xmin + 1), dtype=np.uint8)

        contour[:, 0] = contour[:, 0] - xmin
        contour[:, 1] = contour[:, 1] - ymin

        cv2.fillPoly(mask, contour.reshape(1, -1, 2).astype("int32"), 1)
        return cv2.mean(bitmap[ymin:ymax + 1, xmin:xmax + 1], mask)[0]

    def __call__(self, pred_array, shape_info):
        """
        Main postprocessing function for ONNX
        
        COMPLETE HAND-TRACED PIPELINE:
        ================================
        
        INPUT EXAMPLE:
        - pred_array: (1,1,320,480) ONNX detection output v·ªõi probability values
        - shape_info: [0.8, 0.75] ratios t·ª´ preprocessing
        
        STEP-BY-STEP TRANSFORMATION:
        
        1. INPUT HANDLING:
           pred_array(1,1,320,480) ‚Üí pred(320,480) 
           Remove batch v√† channel dimensions
        
        2. BINARY THRESHOLDING:
           pred[100,150] = 0.85 > 0.3 ‚Üí segmentation[100,150] = True
           pred[200,250] = 0.15 ‚â§ 0.3 ‚Üí segmentation[200,250] = False
           
        3. SHAPE CALCULATION:
           shape_info = [0.8, 0.75] ‚Üí ratio_h=0.8, ratio_w=0.75
           src_h = 320/0.8 = 400, src_w = 480/0.75 = 640 (original image size)
           
        4. CONTOUR DETECTION:
           segmentation ‚Üí cv2.findContours ‚Üí contours list
           
        5. BOX EXTRACTION & FILTERING:
           contour ‚Üí mini_box ‚Üí score_check ‚Üí unclip ‚Üí scale ‚Üí final_box
           
        6. OUTPUT:
           boxes: [[x1,y1,x2,y2,x3,y3,x4,y4], ...] in original image coordinates
           scores: [0.85, 0.79, ...] confidence values
        """
        
        # STEP 1: Handle ONNX output format
        # ONNX models output tensor v·ªõi batch dimension, c·∫ßn remove ƒë·ªÉ x·ª≠ l√Ω
        if pred_array.ndim == 4:
            pred = pred_array[0, 0, :, :]  # (1,1,H,W) ‚Üí (H,W)
        elif pred_array.ndim == 3:
            pred = pred_array[0, :, :]     # (1,H,W) ‚Üí (H,W)
        else:
            pred = pred_array              # Already (H,W)
            
        # HAND-TRACE: pred_array(1,1,320,480) ‚Üí pred(320,480)
            
        # STEP 2: Create binary mask t·ª´ probability map
        # Pixels > thresh th√†nh white (True), pixels ‚â§ thresh th√†nh black (False)
        segmentation = pred > self.thresh
        
        # HAND-TRACE: thresh=0.3
        # pred values: [[0.1, 0.85, 0.7], [0.2, 0.9, 0.15], ...]
        # segmentation: [[False, True, True], [False, True, False], ...]
        
        # STEP 3: Calculate original image dimensions t·ª´ shape_info
        if len(shape_info) == 2:
            # Format: [ratio_h, ratio_w] from preprocessing
            ratio_h, ratio_w = shape_info
            src_h = int(pred.shape[0] / ratio_h)  # Original height
            src_w = int(pred.shape[1] / ratio_w)  # Original width
        elif len(shape_info) == 4:
            # Format: [src_h, src_w, ratio_h, ratio_w] from PaddleOCR
            src_h, src_w, ratio_h, ratio_w = shape_info
        else:
            raise ValueError(f"Invalid shape_info format: {shape_info}")
        
        # HAND-TRACE: shape_info=[0.8, 0.75]
        # pred.shape = (320, 480)
        # src_h = 320/0.8 = 400, src_w = 480/0.75 = 640
        # Original image was 400x640, ƒë∆∞·ª£c resize th√†nh 320x480 cho detection
        
        # STEP 4: Apply dilation if enabled (optional)
        # Dilation l√†m d√†y c√°c v√πng white trong binary mask
        # Gi√∫p connect c√°c text regions b·ªã disconnect nh·ªè l·∫ª
        if self.dilation_kernel is not None:
            mask = cv2.dilate(
                np.array(segmentation).astype(np.uint8),
                self.dilation_kernel
            )
        else:
            mask = segmentation
        
        # HAND-TRACE: use_dilation=False ‚Üí mask = segmentation (kh√¥ng thay ƒë·ªïi)
        
        # STEP 5: Extract boxes t·ª´ binary mask
        if self.box_type == 'poly':
            # Polygon boxes (c√≥ th·ªÉ c√≥ nhi·ªÅu h∆°n 4 points)
            boxes, scores = self.polygons_from_bitmap(pred, mask, src_w, src_h)
        elif self.box_type == 'quad':
            # Quadrilateral boxes (exactly 4 points)
            boxes, scores = self.boxes_from_bitmap(pred, mask, src_w, src_h)
        else:
            raise ValueError("box_type can only be one of ['quad', 'poly']")
        
        # HAND-TRACE: box_type='quad'
        # Call boxes_from_bitmap(pred(320,480), mask(320,480), 640, 400)
        # Return: boxes in original image coordinates (640x400)
        # boxes = [[[120,50,253,50,253,112,120,112]]], scores = [0.85]
        
        # FINAL OUTPUT:
        # boxes: list of detected text boxes in original image coordinates
        # scores: confidence scores t∆∞∆°ng ·ª©ng v·ªõi m·ªói box
        return boxes, scores


def test_db_postprocess_onnx():
    """
    Test function for DB postprocessing
    
    COMPLETE WORKFLOW DEMONSTRATION:
    ===============================
    
    Simulates real ONNX detection output v√† demonstrates complete pipeline
    t·ª´ detection map ‚Üí final text boxes
    """
    print("=" * 60)
    print("TESTING DB POSTPROCESSING FOR ONNX")
    print("=" * 60)
    
    # STEP 1: Create realistic ONNX detection output
    # Real ONNX model s·∫Ω output probability map cho text regions
    H, W = 320, 480  # Detection resolution
    fake_detection = np.zeros((1, 1, H, W), dtype=np.float32)
    
    # SIMULATE TEXT REGIONS v·ªõi different confidence levels:
    # Region 1: High confidence text "HELLO"
    fake_detection[0, 0, 50:100, 100:200] = 0.8   # Strong text signal
    # Region 2: Medium confidence text "WORLD"  
    fake_detection[0, 0, 150:180, 250:400] = 0.9  # Very strong signal
    # Region 3: Low confidence text/noise
    fake_detection[0, 0, 220:270, 50:180] = 0.7   # Medium signal
    
    # Add realistic noise ƒë·ªÉ simulate real model output
    noise = np.random.normal(0, 0.1, fake_detection.shape)
    fake_detection += noise
    fake_detection = np.clip(fake_detection, 0, 1)  # Keep trong [0,1] range
    
    print(f"üìä Simulated ONNX detection output:")
    print(f"   Shape: {fake_detection.shape}")
    print(f"   Value range: [{fake_detection.min():.3f}, {fake_detection.max():.3f}]")
    print(f"   Text regions: 3 simulated areas with different confidence")
    
    # STEP 2: Prepare shape_info from preprocessing
    # Corresponds to: original(400x640) ‚Üí resized(320x480)
    # ratio_h = 320/400 = 0.8, ratio_w = 480/640 = 0.75
    shape_info = [0.8, 0.75]  # [ratio_h, ratio_w]
    
    print(f"üìê Shape info: {shape_info}")
    print(f"   Original image: 400x640 ‚Üí Detection: 320x480")
    print(f"   Scaling ratios: height=0.8, width=0.75")
    
    # STEP 3: Initialize postprocessor v·ªõi PP-OCRv5 compatible settings
    postprocessor = DBPostProcessONNX(
        thresh=0.3,           # Binary threshold
        box_thresh=0.7,       # Confidence threshold  
        max_candidates=1000,  # Max contours
        unclip_ratio=2.0,     # Box expansion
        score_mode="fast",    # Fast scoring method
        box_type='quad'       # Quadrilateral boxes
    )
    
    print(f"‚öôÔ∏è  Postprocessor settings:")
    print(f"   Binary threshold: {postprocessor.thresh}")
    print(f"   Confidence threshold: {postprocessor.box_thresh}")
    print(f"   Unclip ratio: {postprocessor.unclip_ratio}")
    
    # STEP 4: Run complete postprocessing pipeline
    print(f"\nüîÑ Running DB postprocessing pipeline...")
    boxes, scores = postprocessor(fake_detection, shape_info)
    
    # STEP 5: Analyze results
    print(f"\nüìã PIPELINE RESULTS:")
    print(f"   Input: Detection map {fake_detection.shape}")
    print(f"   Binary threshold: pixels > {postprocessor.thresh}")
    print(f"   Contours found: {len(boxes)} (after filtering)")
    print(f"   Final boxes: {len(boxes)} detected text regions")
    
    if len(boxes) > 0:
        print(f"\nüì¶ DETECTED BOXES:")
        for i, (box, score) in enumerate(zip(boxes, scores)):
            print(f"   Box {i+1}: {box.tolist()}")
            print(f"           Score: {score:.4f}")
            print(f"           Size: {abs(box[2]-box[0])}x{abs(box[5]-box[1])} pixels")
    else:
        print(f"   ‚ö†Ô∏è  No boxes detected (all filtered out)")
    
    print("\n‚úÖ DB postprocessing test completed!")
    print("   This demonstrates complete workflow: ONNX output ‚Üí text boxes")
    return boxes, scores


def compare_formats():
    """
    Compare PaddleOCR vs ONNX input formats
    """
    print("\n" + "=" * 60)
    print("COMPARING PADDLEOCR VS ONNX FORMATS")
    print("=" * 60)
    
    print("PaddleOCR format:")
    print("  Input: outs_dict = {'maps': paddle.Tensor}")
    print("  Shape info: [src_h, src_w, ratio_h, ratio_w]")
    print("  Usage: postprocess(outs_dict, shape_list)")
    
    print("\nONNX format:")
    print("  Input: pred_array = numpy.ndarray (1, 1, H, W)")
    print("  Shape info: [ratio_h, ratio_w]")
    print("  Usage: postprocess(pred_array, shape_info)")
    
    print("\nüîÑ Key differences:")
    print("  1. No Paddle Tensor dependency")
    print("  2. Direct numpy array input")
    print("  3. Simplified shape_info format")
    print("  4. Same core algorithm and parameters")


if __name__ == "__main__":
    # Test the ONNX postprocessing
    test_db_postprocess_onnx()
    
    # Compare formats
    compare_formats()
    
    print(f"\nüéØ SUMMARY:")
    print(f"‚úÖ DB postprocessing successfully converted to ONNX")
    print(f"‚úÖ Compatible with numpy arrays from ONNX inference")
    print(f"‚úÖ Same detection quality as original PaddleOCR")
    print(f"‚úÖ Ready for integration with ONNX pipeline")